# Entrophy and Information Gain

## 엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.

## Reference

<a href="https://eehoeskrap.tistory.com/13">https://eehoeskrap.tistory.com/13</a>

### Entrophy (0에서 1까지의 값)

주어진 데이터 집합의 혼잡도

<img src="https://latex.codecogs.com/svg.latex?Entrophy(S)=-\overset{m}{\underset{i=1}{\sum}}p_i{log_2}(p_i);\ p_i=\frac{freq(C_i,S)}{|S|}" title="Entrophy(S)=-\overset{m}{\underset{i=1}{\sum}}p_i{log_2}(p_i);\ p_i=\frac{freq(C_i,S)}{|S|}" />

(S: 주어진 데이터들의 집합, C: 레코드(클래스) 값들의 집합, freq(Ci,S):S에서 Ci에 속하는 레코드의 수, |S|: 주어진 데이터들의 집합의 데이터 개수)

주어진 데이터 집합에서,
* 서로 다른 종류의 레코드들이 섞여있으면 --- 1에 근접
* 서로 같은 종류의 레코드들이 섞여있으면 --- 0에 근접

### Information Gain

어떤 속성을 선택함으로써 데이터를 더 잘 구분하게 되는 것

<img src="https://latex.codecogs.com/svg.latex?Gain(A)=I(s_1,s_2,...,s_m)-E(attribute\ A)" title="Gain(A)=I(s_1,s_2,...,s_m)-E(attribute\ A)" />

즉, 정보이득의 계산은 상위노드의 엔트로피에서 하위 노드의 엔트로피를 뺀 값이다. E(A)는 A라는 속성을 선택했을 때 하위로 작은 m개의 노드로 나누어진다고 하면 하위 각 노드의 엔트로피를 계산한 후 노드의 속한 레코드의 개수를 가중치로 하여 엔트로피를 평균한 값이다.

즉 Gain(A)는 속성 A를 선택했을 때의 정보이득 양을 계산하는 수식으로 원래 노드의 엔트로피를 구하고, 방금 구한 엔트로피를 선택한 후의 m개의 하위 노드로 나누어진 것에 대한 전체적인 엔트로피를 구한 후의 값을 뺀 결과이다.

값이 클수록 변별력이 좋으며, 작을 수록 변별력이 나쁘다.